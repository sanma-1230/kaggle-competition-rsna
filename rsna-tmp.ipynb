{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TMP notebook\ncompetition : RSNA Screening Mammography Breast Cancer Detection  \nurl : https://www.kaggle.com/competitions/rsna-breast-cancer-detection","metadata":{}},{"cell_type":"markdown","source":"# 2023/1/22\nテーブルデータのみのlightgbmでどれくらい精度出るか検証  \n* 交差検証なし、パラメータチューニングなし\n\n## 結果\n* LB - 0.04","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import lightgbm as lgb\n# from sklearn.metrics import f1_score\n# from sklearn.model_selection import train_test_split\n# import warnings\n# warnings.simplefilter('ignore')\n\n# def pre_view(df):\n#     if 'view' in df.columns.tolist():\n#         df['view'] = df['view'].apply(lambda x: x if x=='CC' or x=='MLO' else 'others')\n#     else:\n#         pass\n#     return df\n\n# train = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/train.csv')\n# test = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/test.csv')\n# submit = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/sample_submission.csv')\n\n# column = ['laterality', 'view', 'age', 'implant', 'cancer']\n# new_train = train[column]\n# new_train = new_train.dropna()\n# new_train = pre_view(new_train)\n# new_train_dum = pd.get_dummies(new_train)\n# train_cancer = new_train_dum[new_train_dum.cancer==1]\n# train_no_cancer = new_train_dum[new_train_dum.cancer==0]\n# tmp = train_no_cancer.sample(n=1158, random_state=0)\n# concat_train = pd.concat([train_cancer, tmp])\n\n# concat_train = concat_train.reset_index()\n# X = concat_train.drop(columns=['cancer'])\n# y = concat_train[['cancer']]\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n# # X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=0, stratify=y_test)\n\n# model = lgb.LGBMClassifier(random_state=0)\n# model.fit(X_train, y_train)\n# pred = model.predict(X_test)\n# print(f1_score(y_test, pred))\n\n# tmp = test[['laterality', 'view', 'age', 'implant']]\n# tmp = pre_view(tmp)\n# tmp_ = pd.concat([new_train, tmp])\n# tmp_dum = pd.get_dummies(tmp_)\n# tmp_dum = tmp_dum.reset_index()\n# test_X = tmp_dum.iloc[new_train.shape[0]:]\n# test_X = test_X.drop(columns=['cancer'])\n# test_pred = model.predict(test_X)\n# print(test_pred)\n# test_copy = test.copy()\n# test_copy['pred'] = test_pred\n# tmp = test_copy.groupby('prediction_id')['pred'].mean()\n# sub = pd.DataFrame(data={'prediction_id': tmp.index.tolist(), 'cancer': tmp.values.tolist()})\n# display(sub)\n# sub.to_csv('submission.csv', index=None)","metadata":{"execution":{"iopub.status.busy":"2023-01-22T10:37:03.542591Z","iopub.execute_input":"2023-01-22T10:37:03.543201Z","iopub.status.idle":"2023-01-22T10:37:03.856648Z","shell.execute_reply.started":"2023-01-22T10:37:03.543164Z","shell.execute_reply":"2023-01-22T10:37:03.855913Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2023/1/28\n画像について調べる","metadata":{}},{"cell_type":"code","source":"# !pip install -qU python-gdcm pydicom pylibjpeg\n# !pip install japanize-matplotlib\n!pip install /kaggle/input/dicomsdl-offline-installer/dicomsdl-0.109.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\nimport dicomsdl\nimport cv2\nimport os\nimport copy\nimport random\nimport pydicom\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n# import japanize_matplotlib\nimport matplotlib.pyplot as plt\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 50)\n\nfrom torch.utils.data import Dataset, DataLoader\n#　使い方\n# https://pystyle.info/pytorch-how-to-create-custom-dataset-class/\nimport torch","metadata":{"execution":{"iopub.status.busy":"2023-01-28T08:29:47.452923Z","iopub.execute_input":"2023-01-28T08:29:47.453296Z","iopub.status.idle":"2023-01-28T08:30:17.074436Z","shell.execute_reply.started":"2023-01-28T08:29:47.453263Z","shell.execute_reply":"2023-01-28T08:30:17.073165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/train.csv')\ndicom_df = pd.read_csv('/kaggle/input/rsna-dicom-csv/dicom.csv')\ncustom_train = pd.read_csv('/kaggle/input/rsnacustomtrain/custom_train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-01-28T08:30:17.077259Z","iopub.execute_input":"2023-01-28T08:30:17.077558Z","iopub.status.idle":"2023-01-28T08:30:17.817632Z","shell.execute_reply.started":"2023-01-28T08:30:17.077530Z","shell.execute_reply":"2023-01-28T08:30:17.816653Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_image = '/kaggle/input/rsna-breast-cancer-detection/train_images/10011/1031443799.dcm'\n# im_data = pydicom.dcmread(sample_image)\n# # print(im_data)\n# data = im_data.pixel_array\n# data.shape\n# plt.imshow(im_data.pixel_array)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-28T06:38:01.754762Z","iopub.execute_input":"2023-01-28T06:38:01.755274Z","iopub.status.idle":"2023-01-28T06:38:01.761272Z","shell.execute_reply.started":"2023-01-28T06:38:01.755225Z","shell.execute_reply":"2023-01-28T06:38:01.760006Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(15, 10))\n# axs = axs.flatten()\n# for i in range(8):\n#     if i <= 3:\n#         MONO = 'MONOCHROME1'\n#     else:\n#         MONO = 'MONOCHROME2'\n#     ran = random.randint(0,9000)\n#     tmp = dicom_df[dicom_df.PhotometricInterpretation == MONO].iloc[ran]\n#     p_id = tmp.patient_id\n#     i_id = tmp.image_id\n#     pas = f'/kaggle/input/rsna-breast-cancer-detection/train_images/{p_id}/{i_id}.dcm'\n#     im_data = pydicom.dcmread(pas)\n#     axs[i].imshow(im_data.pixel_array)\n#     axs[i].axis('off')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-28T02:14:48.473237Z","iopub.execute_input":"2023-01-28T02:14:48.473956Z","iopub.status.idle":"2023-01-28T02:15:07.620024Z","shell.execute_reply.started":"2023-01-28T02:14:48.473918Z","shell.execute_reply":"2023-01-28T02:15:07.618835Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_image(paths, side='left', size=512, threshold=0.05):\n    dicom_data = pydicom.dcmread(paths)\n    data = np.array(dicom_data.pixel_array)\n    data = data - np.min(data)\n    data = data / np.max(data)\n    if dicom_data.PhotometricInterpretation == \"MONOCHROME1\":\n        data = 1.0 - data\n    image = data[5:-5, 5:-5]\n\n    ret, thresh = cv2.threshold(image, threshold, 1, 0)\n\n    width = image.shape[1]\n    # take all columns up to half image (in width), sumarize them and compare with other half\n    if sum(sum(thresh[:, :width // 2])) > sum(sum(thresh[:, width // 2:])): \n        image_side = 'left'\n    else:\n        image_side = 'right'\n\n    if image_side != side: \n        image = cv2.flip(image, 1)\n    output= cv2.connectedComponentsWithStats((image > 0.05).astype(np.uint8)[:, :], 8, cv2.CV_32S)\n    stats = output[2] # left, top, width, height, area_size\n\n    idx = stats[1:, 4].argmax() + 1\n    x1, y1, w, h = stats[idx][:4]\n    x2 = x1 + w\n    y2 = y1 + h\n\n    image = image[y1: y2, x1: x2]\n    image = cv2.resize(image, (size, size))\n    return image","metadata":{"execution":{"iopub.status.busy":"2023-01-28T08:48:44.274589Z","iopub.execute_input":"2023-01-28T08:48:44.274990Z","iopub.status.idle":"2023-01-28T08:48:44.286369Z","shell.execute_reply.started":"2023-01-28T08:48:44.274954Z","shell.execute_reply":"2023-01-28T08:48:44.285204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# paths = [custom_train.filename[1]]\n# data = transform_image(paths)\n# plt.imshow(data[0])","metadata":{"execution":{"iopub.status.busy":"2023-01-28T09:56:31.025795Z","iopub.execute_input":"2023-01-28T09:56:31.026741Z","iopub.status.idle":"2023-01-28T09:56:31.031256Z","shell.execute_reply.started":"2023-01-28T09:56:31.026703Z","shell.execute_reply":"2023-01-28T09:56:31.030229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path入力\n# 余計な余白除去と左向き揃えをした画像が出力される\n# リサイズ済み\nclass Image_preprocessor:\n    def __init__(self,side='left', size=512):\n        self.side = side\n        self.size = size\n        \n    def preprocess(self, paths):\n        images = []\n        for image_path in paths:\n            image = self.read_xray(image_path)\n            image = self.crop_image(image)\n            img_side = self.determine_side(image)\n            if img_side != self.side: \n                image = cv2.flip(image, 1)\n            image = self.img2roi(image)\n            image = cv2.resize(image, (self.size, self.size))\n            images.append(image)\n        return images\n    \n    \n    def read_xray(self, path, fix_monochrome = True):\n        dicom = dicomsdl.open(path)\n        data = dicom.pixelData(storedvalue=False)  # storedvalue = True for int16 return otherwise float32\n        data = data - np.min(data)\n        data = data / np.max(data)\n        if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n            data = 1.0 - data\n        return data\n\n    def crop_image(self, image):\n        # 画像によっては不要な枠があるので、取り除く\n        image = image[5:-5, 5:-5]\n        return image\n    \n    def img2roi(self, image):\n        output= cv2.connectedComponentsWithStats((image > 0.05).astype(np.uint8)[:, :], 8, cv2.CV_32S)\n        stats = output[2] # left, top, width, height, area_size\n\n        idx = stats[1:, 4].argmax() + 1\n        x1, y1, w, h = stats[idx][:4]\n        x2 = x1 + w\n        y2 = y1 + h\n\n        image_fit = image[y1: y2, x1: x2]\n\n        return image_fit\n    \n    def determine_side(self, img, threshold = 0.05):\n        \"\"\"\n        img: input image\n        threshold: for binirizing image, should be 5\n        Side is determined simply by finding more white side of the image.\n        \"\"\"\n\n        if img.dtype == 'float32':\n            ret, thresh = cv2.threshold(img, threshold, 1, 0)\n#         else:\n#             img = (255*img).astype(dtype = 'float32')\n#             ret, thresh = cv2.threshold(img, threshold, 1, 0)\n\n        width = img.shape[1]\n        # take all columns up to half image (in width), sumarize them and compare with other half\n        if sum(sum(thresh[:, :width // 2])) > sum(sum(thresh[:, width // 2:])): \n            return 'left'\n        else:\n            return 'right'","metadata":{"execution":{"iopub.status.busy":"2023-01-28T08:06:32.142432Z","iopub.execute_input":"2023-01-28T08:06:32.143038Z","iopub.status.idle":"2023-01-28T08:06:32.157763Z","shell.execute_reply.started":"2023-01-28T08:06:32.142985Z","shell.execute_reply":"2023-01-28T08:06:32.156765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# paths = ['/kaggle/input/rsna-breast-cancer-detection/train_images/10006/1864590858.dcm', '/kaggle/input/rsna-breast-cancer-detection/train_images/10006/462822612.dcm']\n# x = Image_preprocessor()\n# data = x.preprocess(paths)\n# plt.imshow(data[1])\n# plt.show()\n# print(data[0])","metadata":{"execution":{"iopub.status.busy":"2023-01-28T09:56:38.337426Z","iopub.execute_input":"2023-01-28T09:56:38.337790Z","iopub.status.idle":"2023-01-28T09:56:38.342364Z","shell.execute_reply.started":"2023-01-28T09:56:38.337759Z","shell.execute_reply":"2023-01-28T09:56:38.341283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = train.copy()\n# root_dir = '/kaggle/input/rsna-breast-cancer-detection/train_images'\n# df['filename'] = root_dir+'/'+df.patient_id.astype(str)+'/'+df.image_id.astype(str)+'.dcm'\n# df['site_id_1'] = df['site_id_2'] = 0\n# df['view_MLO'] = df['view_CC'] = df['view_AT'] = df['view_LM'] = df['view_ML'] = df['view_LMO'] = 0\n# df['machine_49'] = df['machine_48'] = df['machine_29'] = df['machine_21'] = df['machine_93'] = df['machine_216'] = df['machine_210'] = df['machine_170'] = df['machine_190'] = df['machine_197'] = 0\n# for i in range(df.shape[0]):\n#     tmp = df.iloc[i]\n#     s_value = tmp['site_id']\n#     v_value = tmp['view']\n#     m_value = tmp['machine_id']\n#     s_name = f'site_id_{s_value}'\n#     v_name = f'view_{v_value}'\n#     m_name = f'machine_{m_value}'\n#     df.loc[i, s_name] = 1\n#     df.loc[i, v_name] = 1\n#     df.loc[i, m_name] = 1\n# train_input = df.drop(['site_id', 'patient_id', 'image_id', 'laterality', 'view', 'cancer', 'biopsy', 'filename', 'invasive', 'BIRADS', 'density', 'machine_id', 'difficult_negative_case'],axis=1)\n# train_input['age'].fillna(60, inplace=True)\n# train_input.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-01-28T07:13:13.541126Z","iopub.execute_input":"2023-01-28T07:13:13.541807Z","iopub.status.idle":"2023-01-28T07:13:13.546872Z","shell.execute_reply.started":"2023-01-28T07:13:13.541771Z","shell.execute_reply":"2023-01-28T07:13:13.545705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pre_table(df, root_dir='/kaggle/input/rsna-breast-cancer-detection/train_images'):\n    df['filename'] = root_dir+'/'+df.patient_id.astype(str)+'/'+df.image_id.astype(str)+'.dcm'\n    df['site_id_1'] = df['site_id_2'] = 0\n    df['view_MLO'] = df['view_CC'] = df['view_AT'] = df['view_LM'] = df['view_ML'] = df['view_LMO'] = 0\n    df['machine_49'] = df['machine_48'] = df['machine_29'] = df['machine_21'] = df['machine_93'] = df['machine_216'] = df['machine_210'] = df['machine_170'] = df['machine_190'] = df['machine_197'] = 0\n    for i in range(df.shape[0]):\n        tmp = df.iloc[i]\n        s_value = tmp['site_id']\n        v_value = tmp['view']\n        m_value = tmp['machine_id']\n        s_name = f'site_id_{s_value}'\n        v_name = f'view_{v_value}'\n        m_name = f'machine_{m_value}'\n        df.loc[i, s_name] = 1\n        df.loc[i, v_name] = 1\n        df.loc[i, m_name] = 1\n    train_input = df.drop(['site_id', 'patient_id', 'image_id', 'laterality', 'view', 'cancer', 'biopsy', 'filename', 'invasive', 'BIRADS', 'density', 'machine_id', 'difficult_negative_case'], axis=1)\n    train_input['age'].fillna(60, inplace=True)\n    return train_input\n\n# t = Image_preprocessor()\nclass RSNADataset(Dataset):\n    def __init__(self, df, root_dir, is_preprocess=False, transform=False):\n        self.table = df\n        self.root_dir = root_dir\n        self.is_preprocess = is_preprocess\n        self.transform = transform\n        \n    def __len__(self):\n        return self.table.shape[0]\n    \n    def __getitem__(self, idx):\n        df_tmp = self.table\n        if self.is_preprocess:\n            train_dataset = pre_table(df_tmp, self.root_dir)\n        else:\n            train_dataset = self.table.drop(['site_id', 'patient_id', 'image_id', 'laterality', 'view', 'cancer', 'biopsy', 'filename', 'invasive', 'BIRADS', 'density', 'machine_id', 'difficult_negative_case'], axis=1)\n        image_name = df_tmp.filename.tolist()[idx]\n        image = t.preprocess(image_name)\n        image = torch.Tensor(image)\n        target = torch.Tensor(df_tmp['cancer'].tolist())[idx]\n        sample = {'image': image,'answer':target}\n        return sample\n        \n    ","metadata":{"execution":{"iopub.status.busy":"2023-01-28T08:00:13.394164Z","iopub.execute_input":"2023-01-28T08:00:13.394596Z","iopub.status.idle":"2023-01-28T08:00:13.409452Z","shell.execute_reply.started":"2023-01-28T08:00:13.394561Z","shell.execute_reply":"2023-01-28T08:00:13.408188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# csv_file = '/kaggle/input/rsnacustomtrain/custom_train.csv'\n# root_dir = '/kaggle/inpu/rsna-breast-cancer-detection/train_images'\n# dataset = RSNADataset(custom_train, root_dir)","metadata":{"execution":{"iopub.status.busy":"2023-01-28T07:48:19.192882Z","iopub.execute_input":"2023-01-28T07:48:19.193491Z","iopub.status.idle":"2023-01-28T07:48:19.202365Z","shell.execute_reply.started":"2023-01-28T07:48:19.193446Z","shell.execute_reply":"2023-01-28T07:48:19.201182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import datasets\nimport numpy as np\nimport pandas as pd\nfrom torch import optim\nimport torch\nCUDA_LAUNCH_BLOCKING=1\nfrom torchvision import models\nimport torch.nn as nn\nimport os\nfrom skimage import io, transform\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport skimage\nfrom skimage.color import rgb2gray, gray2rgb\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2023-01-28T08:51:06.597093Z","iopub.execute_input":"2023-01-28T08:51:06.597467Z","iopub.status.idle":"2023-01-28T08:51:07.251636Z","shell.execute_reply.started":"2023-01-28T08:51:06.597439Z","shell.execute_reply":"2023-01-28T08:51:07.250660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        paths = self.df.filename.tolist()[idx]\n        data = transform_image(paths)\n        data = torch.Tensor(data)\n        target = torch.Tensor(self.df['cancer'].tolist())[idx]\n        sample = {'data': data, 'target': target}\n        return sample\n    ","metadata":{"execution":{"iopub.status.busy":"2023-01-28T09:48:54.644018Z","iopub.execute_input":"2023-01-28T09:48:54.644482Z","iopub.status.idle":"2023-01-28T09:48:54.658893Z","shell.execute_reply.started":"2023-01-28T09:48:54.644444Z","shell.execute_reply":"2023-01-28T09:48:54.657585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset = TrainDataset(custom_train)\n# dataloader = DataLoader(dataset, batch_size=10)","metadata":{"execution":{"iopub.status.busy":"2023-01-28T09:51:38.538837Z","iopub.execute_input":"2023-01-28T09:51:38.539728Z","iopub.status.idle":"2023-01-28T09:51:38.545690Z","shell.execute_reply.started":"2023-01-28T09:51:38.539681Z","shell.execute_reply":"2023-01-28T09:51:38.544571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for batch in dataloader:\n#     print(batch['data'].shape)\n#     print(batch['target'])","metadata":{"execution":{"iopub.status.busy":"2023-01-28T09:54:18.226761Z","iopub.execute_input":"2023-01-28T09:54:18.227177Z","iopub.status.idle":"2023-01-28T09:54:18.232252Z","shell.execute_reply.started":"2023-01-28T09:54:18.227141Z","shell.execute_reply":"2023-01-28T09:54:18.230845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\nimport timm\nfrom pprint import pprint\npprint(timm.list_models(pretrained = True))","metadata":{"execution":{"iopub.status.busy":"2023-01-28T09:48:58.272794Z","iopub.execute_input":"2023-01-28T09:48:58.273798Z","iopub.status.idle":"2023-01-28T09:48:58.292477Z","shell.execute_reply.started":"2023-01-28T09:48:58.273758Z","shell.execute_reply":"2023-01-28T09:48:58.291411Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tmp_model = timm.create_model(model_name='resnest26d')","metadata":{"execution":{"iopub.status.busy":"2023-01-28T09:48:58.308153Z","iopub.execute_input":"2023-01-28T09:48:58.308720Z","iopub.status.idle":"2023-01-28T09:48:58.583531Z","shell.execute_reply.started":"2023-01-28T09:48:58.308682Z","shell.execute_reply":"2023-01-28T09:48:58.582462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\nclass Effnet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(2))\n        self.effnet = timm.create_model(model_name = \"tf_efficientnet_b0\", pretrained = False)\n        n_features = self.effnet.classifier.in_features\n        self.effnet.classifier = nn.Linear(n_features, 1)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.effnet(x)\n        return x\nmodel = Effnet()","metadata":{"execution":{"iopub.status.busy":"2023-01-28T09:48:58.585488Z","iopub.execute_input":"2023-01-28T09:48:58.585864Z","iopub.status.idle":"2023-01-28T09:48:58.704828Z","shell.execute_reply.started":"2023-01-28T09:48:58.585826Z","shell.execute_reply":"2023-01-28T09:48:58.703858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(DEVICE)\nprint(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2023-01-28T09:48:58.706437Z","iopub.execute_input":"2023-01-28T09:48:58.706807Z","iopub.status.idle":"2023-01-28T09:48:58.728464Z","shell.execute_reply.started":"2023-01-28T09:48:58.706772Z","shell.execute_reply":"2023-01-28T09:48:58.727493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters())\n","metadata":{"execution":{"iopub.status.busy":"2023-01-28T09:48:58.730716Z","iopub.execute_input":"2023-01-28T09:48:58.731547Z","iopub.status.idle":"2023-01-28T09:48:58.737794Z","shell.execute_reply.started":"2023-01-28T09:48:58.731521Z","shell.execute_reply":"2023-01-28T09:48:58.736692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = TrainDataset(custom_train)\ndataloader = DataLoader(dataset, batch_size=10)","metadata":{"execution":{"iopub.status.busy":"2023-01-28T09:48:58.739381Z","iopub.execute_input":"2023-01-28T09:48:58.740007Z","iopub.status.idle":"2023-01-28T09:48:58.746471Z","shell.execute_reply.started":"2023-01-28T09:48:58.739972Z","shell.execute_reply":"2023-01-28T09:48:58.745576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.train()\n# for data in dataloader:\n#     optimizer.zero_grad()\n#     X = data['data'].float().to(DEVICE)\n#     y = data['target'].float().to(DEVICE)\n#     pred = model(X)\n#     loss = criterion(pred, y)\n#     loss.backward()\n#     optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2023-01-28T09:54:04.182053Z","iopub.execute_input":"2023-01-28T09:54:04.182435Z","iopub.status.idle":"2023-01-28T09:54:04.188058Z","shell.execute_reply.started":"2023-01-28T09:54:04.182404Z","shell.execute_reply":"2023-01-28T09:54:04.186985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2023/1/29","metadata":{}},{"cell_type":"code","source":"!pip install -U pylibjpeg pylibjpeg-openjpeg pylibjpeg-libjpeg pydicom python-gdcm","metadata":{"execution":{"iopub.status.busy":"2023-01-30T06:19:26.356413Z","iopub.execute_input":"2023-01-30T06:19:26.356856Z","iopub.status.idle":"2023-01-30T06:19:37.417529Z","shell.execute_reply.started":"2023-01-30T06:19:26.356766Z","shell.execute_reply":"2023-01-30T06:19:37.416199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install python-gdcm -q\n!pip install pylibjpeg -q","metadata":{"execution":{"iopub.status.busy":"2023-02-09T06:47:30.492625Z","iopub.execute_input":"2023-02-09T06:47:30.493154Z","iopub.status.idle":"2023-02-09T06:47:49.795934Z","shell.execute_reply.started":"2023-02-09T06:47:30.493113Z","shell.execute_reply":"2023-02-09T06:47:49.794711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport cv2\nimport pydicom\nimport gdcm\nimport pylibjpeg\nimport torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2023-02-09T06:47:52.912070Z","iopub.execute_input":"2023-02-09T06:47:52.912456Z","iopub.status.idle":"2023-02-09T06:47:52.918712Z","shell.execute_reply.started":"2023-02-09T06:47:52.912418Z","shell.execute_reply":"2023-02-09T06:47:52.917696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/train.csv')\ndicom_df = pd.read_csv('/kaggle/input/rsna-dicom-csv/dicom.csv')\ncustom_train = pd.read_csv('/kaggle/input/rsnacustomtrain/custom_train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-01-30T06:19:39.466131Z","iopub.execute_input":"2023-01-30T06:19:39.466795Z","iopub.status.idle":"2023-01-30T06:19:40.303771Z","shell.execute_reply.started":"2023-01-30T06:19:39.466755Z","shell.execute_reply":"2023-01-30T06:19:40.302604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_image(paths, side='left', size=512, threshold=0.05):\n    dicom_data = pydicom.dcmread(paths)\n    data = np.array(dicom_data.pixel_array)\n    data = data - np.min(data)\n    data = data / np.max(data)\n    if dicom_data.PhotometricInterpretation == \"MONOCHROME1\":\n        data = 1.0 - data\n    image = data[5:-5, 5:-5]\n\n    ret, thresh = cv2.threshold(image, threshold, 1, 0)\n\n    width = image.shape[1]\n    # take all columns up to half image (in width), sumarize them and compare with other half\n    if sum(sum(thresh[:, :width // 2])) > sum(sum(thresh[:, width // 2:])): \n        image_side = 'left'\n    else:\n        image_side = 'right'\n\n    if image_side != side: \n        image = cv2.flip(image, 1)\n    output= cv2.connectedComponentsWithStats((image > 0.05).astype(np.uint8)[:, :], 8, cv2.CV_32S)\n    stats = output[2] # left, top, width, height, area_size\n\n    idx = stats[1:, 4].argmax() + 1\n    x1, y1, w, h = stats[idx][:4]\n    x2 = x1 + w\n    y2 = y1 + h\n\n    image = image[y1: y2, x1: x2]\n    image = cv2.resize(image, (size, size))\n    return image","metadata":{"execution":{"iopub.status.busy":"2023-01-30T06:19:45.876274Z","iopub.execute_input":"2023-01-30T06:19:45.876763Z","iopub.status.idle":"2023-01-30T06:19:45.889483Z","shell.execute_reply.started":"2023-01-30T06:19:45.876717Z","shell.execute_reply":"2023-01-30T06:19:45.888204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.df = df\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        print(index)\n        f = self.df.filename.tolist()[index]\n        image = transform_image(f)\n        target = torch.Tensor(self.df.cancer.tolist())[index]\n        image = torch.Tensor(image)\n#         target = target.unsqueeze(dim=-1)\n#         image = image.unsqueeze(dim=0)\n        send = {'image': image, 'target': target}\n        return send","metadata":{"execution":{"iopub.status.busy":"2023-02-10T02:50:44.825237Z","iopub.execute_input":"2023-02-10T02:50:44.825933Z","iopub.status.idle":"2023-02-10T02:50:44.833081Z","shell.execute_reply.started":"2023-02-10T02:50:44.825896Z","shell.execute_reply":"2023-02-10T02:50:44.831921Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"tmp_dataset = TrainDataset(custom_train)\ntmp_dataloader = DataLoader(tmp_dataset, batch_size=30)","metadata":{"execution":{"iopub.status.busy":"2023-02-10T02:50:47.556031Z","iopub.execute_input":"2023-02-10T02:50:47.556405Z","iopub.status.idle":"2023-02-10T02:50:47.561038Z","shell.execute_reply.started":"2023-02-10T02:50:47.556352Z","shell.execute_reply":"2023-02-10T02:50:47.560001Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"for i in tmp_dataloader:\n    print(i)","metadata":{"execution":{"iopub.status.busy":"2023-02-10T02:52:41.919725Z","iopub.execute_input":"2023-02-10T02:52:41.920081Z","iopub.status.idle":"2023-02-10T02:53:05.375707Z","shell.execute_reply.started":"2023-02-10T02:52:41.920050Z","shell.execute_reply":"2023-02-10T02:53:05.372762Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/4131744537.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1805610231.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/3110144962.py\u001b[0m in \u001b[0;36mtransform_image\u001b[0;34m(paths, side, size, threshold)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtransform_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdicom_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydicom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdcmread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicom_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pydicom/dataset.py\u001b[0m in \u001b[0;36mpixel_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m         \"\"\"\n\u001b[0;32m-> 1887\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_pixel_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1888\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numpy.ndarray\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pixel_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pydicom/dataset.py\u001b[0m in \u001b[0;36mconvert_pixel_data\u001b[0;34m(self, handler_name)\u001b[0m\n\u001b[1;32m   1442\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_pixel_data_using_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_pixel_data_without_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_pixel_data_using_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pydicom/dataset.py\u001b[0m in \u001b[0;36m_convert_pixel_data_without_handler\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavailable_handlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1536\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_pixel_data_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1537\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pydicom/dataset.py\u001b[0m in \u001b[0;36m_do_pixel_data_conversion\u001b[0;34m(self, handler)\u001b[0m\n\u001b[1;32m   1561\u001b[0m         \u001b[0;31m# Use the handler to get a 1D numpy array of the pixel data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m         \u001b[0;31m# Will raise an exception if no pixel data element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1563\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pixeldata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1564\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pixel_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshape_pixel_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pydicom/pixel_data_handlers/gdcm_handler.py\u001b[0m in \u001b[0;36mget_pixeldata\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mgdcm_data_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mgdcm_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgdcm_data_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mpixel_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdcm_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mpixel_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_pixel_str_fileio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/_gdcm/gdcmswig.py\u001b[0m in \u001b[0;36mGetBuffer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4891\u001b[0m         \u001b[0mAccess\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mraw\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4892\u001b[0m         \"\"\"\n\u001b[0;32m-> 4893\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_gdcmswig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitmap_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4895\u001b[0m \u001b[0;31m# Register Bitmap in _gdcmswig:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.5)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        \n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\n        self.conv2 = nn.Conv2d(16, 32, 3)\n        \n        self.fc1 = nn.Linear(32*253*253, 100)\n        self.fc2 = nn.Linear(100, 50)\n        self.fc3 = nn.Linear(50, 10)\n        self.fc4 = nn.Linear(10, 1)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pool(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.relu(x)\n        x = x.view(-1, 32*253*253)\n        x = self.fc1(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        x = self.fc3(x)\n        x = self.fc4(x)\n        return x\n    ","metadata":{"execution":{"iopub.status.busy":"2023-01-30T06:32:34.216861Z","iopub.execute_input":"2023-01-30T06:32:34.217616Z","iopub.status.idle":"2023-01-30T06:32:34.229746Z","shell.execute_reply.started":"2023-01-30T06:32:34.217576Z","shell.execute_reply":"2023-01-30T06:32:34.228786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n# device = torch.device(\"cuda:0\")\nnet = Net()\nnet = net.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.005)","metadata":{"execution":{"iopub.status.busy":"2023-01-30T06:32:39.287764Z","iopub.execute_input":"2023-01-30T06:32:39.288790Z","iopub.status.idle":"2023-01-30T06:32:41.368496Z","shell.execute_reply.started":"2023-01-30T06:32:39.288749Z","shell.execute_reply":"2023-01-30T06:32:41.367479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(3):\n    print(f'epoch:{epoch}')\n    for i, data in enumerate(tmp_dataloader):\n        print(f'i:{i}')\n        if i==20:\n            break\n        inputs = data['image']\n        labels = data['target']\n        inputs, labels = inputs.to(device), labels.to(device)\n        inputs = inputs.unsqueeze(dim=1)\n        labels = labels.unsqueeze(dim=1)\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        print(f'train_loss:{loss}')\n\n#     for data in tmp_dataloader:\n#         inputs = data['image']\n#         inputs = inputs.unsqueeze(dim=1)\n#         labels = data['target']\n#         labels = labels.unsqueeze(dim=1)\n#         inputs, labels = inputs.to(device), labels.to(device)\n#         optimizer.zero_grad()\n#         outputs = net(inputs)\n#         loss = criterion(outputs, labels)\n#         print(f'test_loss:{loss}')\n        ","metadata":{"execution":{"iopub.status.busy":"2023-01-30T06:33:00.521541Z","iopub.execute_input":"2023-01-30T06:33:00.522006Z","iopub.status.idle":"2023-01-30T06:54:05.365556Z","shell.execute_reply.started":"2023-01-30T06:33:00.521967Z","shell.execute_reply":"2023-01-30T06:54:05.363845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2023/2/1","metadata":{}},{"cell_type":"markdown","source":"htmlをおしゃれにするやつを見つけたからやってみた","metadata":{}},{"cell_type":"code","source":"!wget http://bit.ly/3ZLyF82 -O CSS.css -q\n    \nfrom IPython.core.display import HTML\nwith open('./CSS.css', 'r') as file:\n    custom_css = file.read()\n\nHTML(custom_css)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:21:58.914929Z","iopub.execute_input":"2023-02-01T10:21:58.915271Z","iopub.status.idle":"2023-02-01T10:22:00.675000Z","shell.execute_reply.started":"2023-02-01T10:21:58.915187Z","shell.execute_reply":"2023-02-01T10:22:00.673669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:23:26.201738Z","iopub.execute_input":"2023-02-01T10:23:26.202209Z","iopub.status.idle":"2023-02-01T10:23:26.308711Z","shell.execute_reply.started":"2023-02-01T10:23:26.202175Z","shell.execute_reply":"2023-02-01T10:23:26.307544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(train.age)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:24:40.803009Z","iopub.execute_input":"2023-02-01T10:24:40.803399Z","iopub.status.idle":"2023-02-01T10:24:41.063581Z","shell.execute_reply.started":"2023-02-01T10:24:40.803365Z","shell.execute_reply":"2023-02-01T10:24:41.062537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.set()\nplt.hist(train.age)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T10:26:06.727444Z","iopub.execute_input":"2023-02-01T10:26:06.728184Z","iopub.status.idle":"2023-02-01T10:26:07.015763Z","shell.execute_reply.started":"2023-02-01T10:26:06.728147Z","shell.execute_reply":"2023-02-01T10:26:07.014678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2023/2/9","metadata":{}},{"cell_type":"code","source":"!pip install -U pylibjpeg pylibjpeg-openjpeg pylibjpeg-libjpeg pydicom python-gdcm","metadata":{"execution":{"iopub.status.busy":"2023-02-10T03:37:37.645679Z","iopub.execute_input":"2023-02-10T03:37:37.646126Z","iopub.status.idle":"2023-02-10T03:37:48.436135Z","shell.execute_reply.started":"2023-02-10T03:37:37.646033Z","shell.execute_reply":"2023-02-10T03:37:48.434888Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pylibjpeg in /opt/conda/lib/python3.7/site-packages (1.4.0)\nRequirement already satisfied: pylibjpeg-openjpeg in /opt/conda/lib/python3.7/site-packages (1.3.1)\nRequirement already satisfied: pylibjpeg-libjpeg in /opt/conda/lib/python3.7/site-packages (1.3.3)\nRequirement already satisfied: pydicom in /opt/conda/lib/python3.7/site-packages (2.3.1)\nRequirement already satisfied: python-gdcm in /opt/conda/lib/python3.7/site-packages (3.0.21)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pylibjpeg) (1.21.6)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport cv2\nimport PIL\nimport pydicom\nimport gdcm\nimport pylibjpeg\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-02-10T03:37:48.438570Z","iopub.execute_input":"2023-02-10T03:37:48.439300Z","iopub.status.idle":"2023-02-10T03:37:51.516827Z","shell.execute_reply.started":"2023-02-10T03:37:48.439250Z","shell.execute_reply":"2023-02-10T03:37:51.515802Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/train.csv')\ndicom_df = pd.read_csv('/kaggle/input/rsna-dicom-csv/dicom.csv')\ncustom_train = pd.read_csv('/kaggle/input/rsnacustomtrain/custom_train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-02-10T03:37:53.095926Z","iopub.execute_input":"2023-02-10T03:37:53.096588Z","iopub.status.idle":"2023-02-10T03:37:53.958069Z","shell.execute_reply.started":"2023-02-10T03:37:53.096548Z","shell.execute_reply":"2023-02-10T03:37:53.956951Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def transform_image(paths, side='left', size=512, threshold=0.05):\n    dicom_data = pydicom.dcmread(paths)\n    data = np.array(dicom_data.pixel_array)\n    data = data - np.min(data)\n    data = data / np.max(data)\n    if dicom_data.PhotometricInterpretation == \"MONOCHROME1\":\n        data = 1.0 - data\n    image = data[5:-5, 5:-5]\n\n    ret, thresh = cv2.threshold(image, threshold, 1, 0)\n\n    width = image.shape[1]\n    # take all columns up to half image (in width), sumarize them and compare with other half\n    if sum(sum(thresh[:, :width // 2])) > sum(sum(thresh[:, width // 2:])): \n        image_side = 'left'\n    else:\n        image_side = 'right'\n\n    if image_side != side: \n        image = cv2.flip(image, 1)\n    output= cv2.connectedComponentsWithStats((image > 0.05).astype(np.uint8)[:, :], 8, cv2.CV_32S)\n    stats = output[2] # left, top, width, height, area_size\n\n    idx = stats[1:, 4].argmax() + 1\n    x1, y1, w, h = stats[idx][:4]\n    x2 = x1 + w\n    y2 = y1 + h\n\n    image = image[y1: y2, x1: x2]\n    image = cv2.resize(image, (size, size))\n    return image","metadata":{"execution":{"iopub.status.busy":"2023-02-10T03:37:54.117271Z","iopub.execute_input":"2023-02-10T03:37:54.118219Z","iopub.status.idle":"2023-02-10T03:37:54.130256Z","shell.execute_reply.started":"2023-02-10T03:37:54.118173Z","shell.execute_reply":"2023-02-10T03:37:54.129167Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.df = df\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        f = self.df.filename.tolist()[index]\n        image = transform_image(f)\n        target = torch.Tensor(self.df.cancer.tolist())[index]\n        image = torch.Tensor(image)\n#         target = target.unsqueeze(dim=-1)\n#         image = image.unsqueeze(dim=0)\n        send = {'image': image, 'target': target}\n        return send","metadata":{"execution":{"iopub.status.busy":"2023-02-10T03:37:55.061104Z","iopub.execute_input":"2023-02-10T03:37:55.061478Z","iopub.status.idle":"2023-02-10T03:37:55.068278Z","shell.execute_reply.started":"2023-02-10T03:37:55.061443Z","shell.execute_reply":"2023-02-10T03:37:55.067272Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"one_x = custom_train[custom_train['cancer']==1]\nzero_x = custom_train[custom_train['cancer']==0]\nprint(one_x.shape)\nprint(zero_x.shape)\nzero_x_2 = zero_x.sample(one_x.shape[0], random_state=0)\nprint(zero_x_2.shape)\nnew_df = pd.concat([one_x, zero_x_2])\nprint(new_df.shape)\nnew_df = new_df.reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-02-10T03:37:56.052983Z","iopub.execute_input":"2023-02-10T03:37:56.053318Z","iopub.status.idle":"2023-02-10T03:37:56.087483Z","shell.execute_reply.started":"2023-02-10T03:37:56.053286Z","shell.execute_reply":"2023-02-10T03:37:56.086430Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(1158, 34)\n(53548, 34)\n(1158, 34)\n(2316, 34)\n","output_type":"stream"}]},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=5, verbose=False, path='checkpoint_model.pth'):\n        self.patience = patience #設定ストップカウンタ\n        self.verbose = verbose #表示の有無\n        self.counter = 0 #現在のカウンタ値\n        self.best_score = None #ベストスコア\n        self.early_stop = False #ストップフラグ\n        self.val_loss_min = np.Inf # 前回のベストスコア記憶用\n        self.path = path #ベストモデルの格納パス\n    \n    def __call__(self, val_loss, model):\n        score = -val_loss\n        \n        if self.best_score is None:\n            self.best_score = score\n            self.checkpoint(val_loss, model)\n        elif score < self.best_score:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.checkpoint(val_loss, model)\n            self.counter = 0\n    \n    def checkpoint(self, val_loss, model):\n        if self.verbose:\n            print(f'validation loss decreased({self.val_loss_min:.6f} ---> {val_loss:.6f}). saving model....')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.5)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        \n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\n        self.conv2 = nn.Conv2d(16, 32, 3)\n        \n        self.fc1 = nn.Linear(32*253*253, 100)\n        self.fc2 = nn.Linear(100, 50)\n        self.fc3 = nn.Linear(50, 10)\n        self.fc4 = nn.Linear(10, 1)\n        \n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pool(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.relu(x)\n        x = x.view(-1, 32*253*253)\n        x = self.fc1(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        x = self.fc3(x)\n        x = self.fc4(x)\n        x = self.sigmoid(x)\n        return x\n    ","metadata":{"execution":{"iopub.status.busy":"2023-02-10T03:37:57.525405Z","iopub.execute_input":"2023-02-10T03:37:57.525792Z","iopub.status.idle":"2023-02-10T03:37:57.541075Z","shell.execute_reply.started":"2023-02-10T03:37:57.525758Z","shell.execute_reply":"2023-02-10T03:37:57.540010Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def train_net(n_epochs, train_loader, net, optimizer, loss_fn, device='cpu'):\n    earlystopping = EarlyStopping(verbose=True)\n    print(f'device=={device}')\n    losses = []\n    net.to(device)\n    \n    for epoch in range(n_epochs):\n        running_loss = 0\n        net.train()\n        print(f'epoch {epoch} start')\n        for index, data in enumerate(train_loader):\n            inputs, labels = data['image'], data['target']\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            inputs = inputs.unsqueeze(dim=1)\n            labels = labels.unsqueeze(dim=1)\n            \n            optimizer.zero_grad()\n            outputs = net(inputs)\n            loss = loss_fn(outputs, labels)\n            print(f'''\n                        index: {index}\n                        outputs: {outputs[:3]}, \n                        labels: {labels[:3]},\n                        loss: {loss}''')\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        losses.append( running_loss / index)\n        print(f'epoch, {epoch} : {running_loss/ index}')\n        \n        earlystopping((running_loss/index), net)\n        if earlystopping.early_stop:\n            print('Early Stop!!!!!')\n            break\n    return losses","metadata":{"execution":{"iopub.status.busy":"2023-02-10T03:37:58.123768Z","iopub.execute_input":"2023-02-10T03:37:58.124458Z","iopub.status.idle":"2023-02-10T03:37:58.135518Z","shell.execute_reply.started":"2023-02-10T03:37:58.124416Z","shell.execute_reply":"2023-02-10T03:37:58.134481Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nnet = Net()\ncriterion = nn.BCELoss()\noptimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.005)","metadata":{"execution":{"iopub.status.busy":"2023-02-10T03:37:59.657736Z","iopub.execute_input":"2023-02-10T03:37:59.658107Z","iopub.status.idle":"2023-02-10T03:38:01.646650Z","shell.execute_reply.started":"2023-02-10T03:37:59.658073Z","shell.execute_reply":"2023-02-10T03:38:01.645614Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train, test = train_test_split(new_df, stratify=new_df.cancer, random_state=0, test_size=0.2)\ndataset = TrainDataset(train)\ndataloader = DataLoader(dataset, batch_size=128, num_workers=2, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-10T03:38:03.243400Z","iopub.execute_input":"2023-02-10T03:38:03.244207Z","iopub.status.idle":"2023-02-10T03:38:03.260038Z","shell.execute_reply.started":"2023-02-10T03:38:03.244162Z","shell.execute_reply":"2023-02-10T03:38:03.259053Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"losses = train_net(n_epochs=3, train_loader=dataloader,  net=net, optimizer=optimizer, loss_fn=criterion, device=device)","metadata":{"execution":{"iopub.status.busy":"2023-02-10T01:20:50.129638Z","iopub.execute_input":"2023-02-10T01:20:50.129985Z","iopub.status.idle":"2023-02-10T02:33:54.709620Z","shell.execute_reply.started":"2023-02-10T01:20:50.129953Z","shell.execute_reply":"2023-02-10T02:33:54.708367Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"device==cuda\nepoch 0 start\n\n                        index: 0\n                        outputs: tensor([[0.4384],\n        [0.4390],\n        [0.4559]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [0.],\n        [1.]], device='cuda:0'),\n                        loss: 0.6999479532241821\n\n                        index: 1\n                        outputs: tensor([[0.4466],\n        [0.4537],\n        [0.4393]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[1.],\n        [1.],\n        [1.]], device='cuda:0'),\n                        loss: 0.703790545463562\n\n                        index: 2\n                        outputs: tensor([[0.4546],\n        [0.4458],\n        [0.4471]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [0.]], device='cuda:0'),\n                        loss: 0.7007441520690918\n\n                        index: 3\n                        outputs: tensor([[0.4322],\n        [0.4487],\n        [0.4457]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [1.]], device='cuda:0'),\n                        loss: 0.6862202882766724\n\n                        index: 4\n                        outputs: tensor([[0.4404],\n        [0.4451],\n        [0.4546]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[1.],\n        [1.],\n        [1.]], device='cuda:0'),\n                        loss: 0.7045772075653076\n\n                        index: 5\n                        outputs: tensor([[0.4419],\n        [0.4532],\n        [0.4389]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [0.]], device='cuda:0'),\n                        loss: 0.6838134527206421\n\n                        index: 6\n                        outputs: tensor([[0.4417],\n        [0.4626],\n        [0.4582]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[1.],\n        [0.],\n        [0.]], device='cuda:0'),\n                        loss: 0.6958748698234558\n\n                        index: 7\n                        outputs: tensor([[0.4494],\n        [0.4522],\n        [0.4451]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [0.],\n        [0.]], device='cuda:0'),\n                        loss: 0.6961178779602051\n\n                        index: 8\n                        outputs: tensor([[0.4611],\n        [0.4603],\n        [0.4705]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[1.],\n        [0.],\n        [1.]], device='cuda:0'),\n                        loss: 0.7066853046417236\n\n                        index: 9\n                        outputs: tensor([[0.4761],\n        [0.4466],\n        [0.4648]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [1.]], device='cuda:0'),\n                        loss: 0.7064306139945984\n\n                        index: 10\n                        outputs: tensor([[0.4785],\n        [0.4785],\n        [0.4741]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [0.],\n        [0.]], device='cuda:0'),\n                        loss: 0.6944789886474609\n\n                        index: 11\n                        outputs: tensor([[0.4785],\n        [0.4709],\n        [0.4694]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [0.],\n        [0.]], device='cuda:0'),\n                        loss: 0.6917850375175476\n\n                        index: 12\n                        outputs: tensor([[0.4771],\n        [0.4789],\n        [0.4701]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [1.]], device='cuda:0'),\n                        loss: 0.6949893236160278\n\n                        index: 13\n                        outputs: tensor([[0.4839],\n        [0.4665],\n        [0.4857]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [0.]], device='cuda:0'),\n                        loss: 0.700088381767273\nepoch, 0 : 0.7511956920990577\nvalidation loss decreased(inf ---> 0.751196). saving model....\nepoch 1 start\n\n                        index: 0\n                        outputs: tensor([[0.4843],\n        [0.4860],\n        [0.5002]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [0.],\n        [1.]], device='cuda:0'),\n                        loss: 0.6941608190536499\n\n                        index: 1\n                        outputs: tensor([[0.4858],\n        [0.4770],\n        [0.4924]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[1.],\n        [1.],\n        [1.]], device='cuda:0'),\n                        loss: 0.6947949528694153\n\n                        index: 2\n                        outputs: tensor([[0.5075],\n        [0.5104],\n        [0.4899]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [0.]], device='cuda:0'),\n                        loss: 0.6957998275756836\n\n                        index: 3\n                        outputs: tensor([[0.4811],\n        [0.4809],\n        [0.5110]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [1.]], device='cuda:0'),\n                        loss: 0.7027778625488281\n\n                        index: 4\n                        outputs: tensor([[0.4857],\n        [0.5064],\n        [0.4822]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[1.],\n        [1.],\n        [1.]], device='cuda:0'),\n                        loss: 0.692288339138031\n\n                        index: 5\n                        outputs: tensor([[0.5168],\n        [0.4611],\n        [0.4954]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [0.]], device='cuda:0'),\n                        loss: 0.6947095990180969\n\n                        index: 6\n                        outputs: tensor([[0.5101],\n        [0.4866],\n        [0.5311]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[1.],\n        [0.],\n        [0.]], device='cuda:0'),\n                        loss: 0.7036527395248413\n\n                        index: 7\n                        outputs: tensor([[0.5019],\n        [0.5061],\n        [0.5316]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [0.],\n        [0.]], device='cuda:0'),\n                        loss: 0.6911632418632507\n\n                        index: 8\n                        outputs: tensor([[0.4967],\n        [0.4965],\n        [0.4731]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[1.],\n        [0.],\n        [1.]], device='cuda:0'),\n                        loss: 0.6965182423591614\n\n                        index: 9\n                        outputs: tensor([[0.4778],\n        [0.4982],\n        [0.5084]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [1.]], device='cuda:0'),\n                        loss: 0.6989994049072266\n\n                        index: 10\n                        outputs: tensor([[0.5207],\n        [0.5000],\n        [0.5131]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [0.],\n        [0.]], device='cuda:0'),\n                        loss: 0.699221670627594\n\n                        index: 11\n                        outputs: tensor([[0.4722],\n        [0.5019],\n        [0.4933]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [0.],\n        [0.]], device='cuda:0'),\n                        loss: 0.6930230259895325\n\n                        index: 12\n                        outputs: tensor([[0.4849],\n        [0.4945],\n        [0.5180]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [1.]], device='cuda:0'),\n                        loss: 0.697140634059906\n\n                        index: 13\n                        outputs: tensor([[0.4887],\n        [0.4902],\n        [0.4731]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [0.]], device='cuda:0'),\n                        loss: 0.6948306560516357\nepoch, 1 : 0.7499293088912964\nvalidation loss decreased(0.751196 ---> 0.749929). saving model....\nepoch 2 start\n\n                        index: 0\n                        outputs: tensor([[0.4775],\n        [0.4868],\n        [0.4691]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [0.],\n        [1.]], device='cuda:0'),\n                        loss: 0.6991031169891357\n\n                        index: 1\n                        outputs: tensor([[0.4942],\n        [0.4982],\n        [0.5093]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[1.],\n        [1.],\n        [1.]], device='cuda:0'),\n                        loss: 0.6910556554794312\n\n                        index: 2\n                        outputs: tensor([[0.4926],\n        [0.4872],\n        [0.4914]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [0.]], device='cuda:0'),\n                        loss: 0.6985511779785156\n\n                        index: 3\n                        outputs: tensor([[0.4807],\n        [0.5009],\n        [0.4908]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [1.]], device='cuda:0'),\n                        loss: 0.6947855353355408\n\n                        index: 4\n                        outputs: tensor([[0.4851],\n        [0.5074],\n        [0.5319]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[1.],\n        [1.],\n        [1.]], device='cuda:0'),\n                        loss: 0.695527970790863\n\n                        index: 5\n                        outputs: tensor([[0.4876],\n        [0.4853],\n        [0.4932]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [0.]], device='cuda:0'),\n                        loss: 0.6915111541748047\n\n                        index: 6\n                        outputs: tensor([[0.4697],\n        [0.4906],\n        [0.4611]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[1.],\n        [0.],\n        [0.]], device='cuda:0'),\n                        loss: 0.694197416305542\n\n                        index: 7\n                        outputs: tensor([[0.4997],\n        [0.4533],\n        [0.4526]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [0.],\n        [0.]], device='cuda:0'),\n                        loss: 0.687763512134552\n\n                        index: 8\n                        outputs: tensor([[0.4563],\n        [0.4728],\n        [0.4596]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[1.],\n        [0.],\n        [1.]], device='cuda:0'),\n                        loss: 0.7004673480987549\n\n                        index: 9\n                        outputs: tensor([[0.4906],\n        [0.4606],\n        [0.4853]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [1.]], device='cuda:0'),\n                        loss: 0.7000000476837158\n\n                        index: 10\n                        outputs: tensor([[0.4768],\n        [0.4985],\n        [0.4655]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [0.],\n        [0.]], device='cuda:0'),\n                        loss: 0.689781904220581\n\n                        index: 11\n                        outputs: tensor([[0.4918],\n        [0.4462],\n        [0.4669]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [0.],\n        [0.]], device='cuda:0'),\n                        loss: 0.6816263198852539\n\n                        index: 12\n                        outputs: tensor([[0.4584],\n        [0.4745],\n        [0.4583]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [1.]], device='cuda:0'),\n                        loss: 0.6940003633499146\n\n                        index: 13\n                        outputs: tensor([[0.4886],\n        [0.4737],\n        [0.4815]], device='cuda:0', grad_fn=<SliceBackward0>), \n                        labels: tensor([[0.],\n        [1.],\n        [0.]], device='cuda:0'),\n                        loss: 0.6952759027481079\nepoch, 2 : 0.7472036480903625\nvalidation loss decreased(0.749929 ---> 0.747204). saving model....\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(list(range(len(losses))), losses)\nplt.ylabel('loss')\nplt.xlabel('epoch')\n","metadata":{"execution":{"iopub.status.busy":"2023-02-10T02:39:47.807522Z","iopub.execute_input":"2023-02-10T02:39:47.807879Z","iopub.status.idle":"2023-02-10T02:39:48.027868Z","shell.execute_reply.started":"2023-02-10T02:39:47.807848Z","shell.execute_reply":"2023-02-10T02:39:48.026935Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Text(0.5, 0, 'epoch')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"model = Net()\nmodel.load_state_dict(torch.load('/kaggle/working/checkpoint_model.pth'))\nmodel.eval()\n# model.to(device)\nwith torch.no_grad():\n    images = []\n    for i in range(test.shape[0]):\n        paths = test.filename.tolist()[i]\n        image = transform_image(paths)\n        image = torch.Tensor(image)\n        images.append(image)\n    images = torch.stack(images)\n    images = images.unsqueeze(dim=1)\n    images.to(device)\n    preds = model(images)\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-02-10T03:38:09.563896Z","iopub.execute_input":"2023-02-10T03:38:09.564283Z","iopub.status.idle":"2023-02-10T03:46:28.540975Z","shell.execute_reply.started":"2023-02-10T03:38:09.564249Z","shell.execute_reply":"2023-02-10T03:46:28.539402Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_11796/2620078542.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_11796/3765212646.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 444\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"],"ename":"RuntimeError","evalue":"Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pfbeta(labels, predictions, beta = 1):\n    y_true_count = 0\n    ctp = 0\n    cfp = 0\n\n    for idx in range(len(labels)):\n        prediction = min(max(predictions[idx], 0), 1)\n        if (labels[idx]):\n            y_true_count += 1\n            ctp += prediction\n        else:\n            cfp += prediction\n\n    beta_squared = beta * beta\n    c_precision = ctp / (ctp + cfp)\n    c_recall = ctp / y_true_count\n    if (c_precision > 0 and c_recall > 0):\n        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n        return result","metadata":{"execution":{"iopub.status.busy":"2023-02-10T03:26:27.170329Z","iopub.execute_input":"2023-02-10T03:26:27.171169Z","iopub.status.idle":"2023-02-10T03:26:27.183428Z","shell.execute_reply.started":"2023-02-10T03:26:27.171067Z","shell.execute_reply":"2023-02-10T03:26:27.182193Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}